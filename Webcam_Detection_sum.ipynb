{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Webcam Detection for traffic signs",
   "id": "36f3d9601e0d5fe9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The TSD (Traffic Signs Detection) was widely used to detect different signs shown while the vehicle is driving and to make sure the vehicle has obeyed the traffic rules. A new TSD model was trained based on a pretrained model-YOLOv8l by using a customized dataset. This dataset contains five different classes of traffic signs (U-turn, Turn left, Turn right, Go straight and Park), after training the TSD model and using mobile phone camera as a input for live testing, this model can successfully detect and classify all five kinds of traffic signs.\n",
    "\n",
    "The following shows the workflow of how to carry out the training and use mobile phone built-in camera for testing."
   ],
   "id": "7627c718bb9add99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Step 1. Process the data"
   ],
   "id": "6424faf800b7818"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Before officially started, the following external packages should be imported for further uses.",
   "id": "323c2f3d00a5e4f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import argparse\n",
    "import cv2\n",
    "import supervision as sv"
   ],
   "id": "636fcb52d08f341c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The original dataset was built to contain 1789 pictures in total. To be more specific, there were 200 pictures for the class U-turn 389 pictures for Turn right class and 400 pictures in each of the rest three classes.\n",
    "\n",
    "The dataset was been splitted into two different datasets which were the train and validation dataset to ensure the correct workflow to train the model.\n",
    "\n",
    "The original dataset should be rearranged like the tree diagram produced by code below."
   ],
   "id": "ec4c19613fbd9644"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_directory_tree(path, indent=\"\"):\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if os.path.isdir(file_path):\n",
    "            print(f\"{indent}├── {file}/\")\n",
    "            print_directory_tree(file_path, indent + \"│   \")\n",
    "        else:\n",
    "            print(f\"{indent}├── {file}\")\n",
    "\n",
    "print(\"dataset/\")\n",
    "print_directory_tree(\"dataset\")"
   ],
   "id": "b3306569a0a5d1f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following code was used to split the dataset and stored in stated direction.",
   "id": "52671c88a5b2427c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Paths to dataset\n",
    "images_path = \"dataset/images\"\n",
    "labels_path = \"dataset/labels\"\n",
    "train_images_path = \"dataset_split/train/images\"\n",
    "train_labels_path = \"dataset_split/train/labels\"\n",
    "val_images_path = \"dataset_split/val/images\"\n",
    "val_labels_path = \"dataset_split/val/labels\"\n",
    "\n",
    "# Create directories for train and validation splits\n",
    "os.makedirs(train_images_path, exist_ok=True) # Use exist_ok=True to make sure no error raised when process again\n",
    "os.makedirs(train_labels_path, exist_ok=True)\n",
    "os.makedirs(val_images_path, exist_ok=True)\n",
    "os.makedirs(val_labels_path, exist_ok=True)\n",
    "\n",
    "# List all image files (end with jpg or png)\n",
    "image_files = [f for f in os.listdir(images_path) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Ensure randomization of the dataset\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Split the dataset\n",
    "split_ratio = 0.8  # 80% training, 20% validation\n",
    "split_index = int(len(image_files) * split_ratio)\n",
    "train_files = image_files[:split_index]\n",
    "val_files = image_files[split_index:]\n",
    "\n",
    "\n",
    "# Copy files to their respective directories\n",
    "def copy_files(files, src_images_path, src_labels_path, dest_images_path, dest_labels_path):\n",
    "    for image_file in files:\n",
    "        # Define source and destination paths for images\n",
    "        src_image = os.path.join(src_images_path, image_file)\n",
    "        dest_image = os.path.join(dest_images_path, image_file)\n",
    "\n",
    "        # Copy the image file\n",
    "        shutil.copy(src_image, dest_image)\n",
    "\n",
    "        # Define source and destination paths for labels\n",
    "        label_file = os.path.splitext(image_file)[0] + \".txt\"  # Match the label file\n",
    "        src_label = os.path.join(src_labels_path, label_file)\n",
    "        dest_label = os.path.join(dest_labels_path, label_file)\n",
    "\n",
    "        # Check if the label file exists before copying\n",
    "        if os.path.exists(src_label):\n",
    "            shutil.copy(src_label, dest_label)\n",
    "        else:\n",
    "            print(f\"Warning: Label file not found for {image_file}\")\n",
    "\n",
    "\n",
    "# Copy the training and validation files\n",
    "copy_files(train_files, images_path, labels_path, train_images_path, train_labels_path)\n",
    "copy_files(val_files, images_path, labels_path, val_images_path, val_labels_path)\n",
    "\n",
    "print(f\"Dataset split completed! Training images: {len(train_files)}, Validation images: {len(val_files)}\")"
   ],
   "id": "e621aeeee5870e38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After ran the code, the splitted data should be stored in a folder called dataset_split and already splitted into train and validation data which can be used for further training.",
   "id": "166dcb8de181673b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2. Train the model",
   "id": "38623de50202dbf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Before start training, the dataset.yaml file was needed to be created and put under the dataset_split folder.\n",
    "\n",
    "This file was mainly used to state the path of train and validation data, and also stated the total class number and their names. The order of classes' names needed to be the same as the one produced when create labels of the pictures (e.g. the class 'U-turn' should have the number of 0 which means it should be putted in the first place).\n",
    "\n",
    "The dataset.yaml file contents were shown below (these contents can only be useful when putted into a yaml file)."
   ],
   "id": "8f8f5ecb2f5fd772"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train: train/images\n",
    "val: val/images\n",
    "\n",
    "nc: 5\n",
    "\n",
    "names: ['U-turn','Turn-right','Turn-left','Park','Go-straight']"
   ],
   "id": "14aa188e320b5a14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After created the yaml file, the dataset_split folder should looked like the tree diagram produced by code below.",
   "id": "6d68cd4aaaccb8cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"dataset_split/\")\n",
    "print_directory_tree(\"dataset_split\")"
   ],
   "id": "822fd8ea54c9b64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, the training can be officially started.\n",
    "\n",
    "The following code was used to create some command-line arguments which can simplify the training process. \n",
    "\n",
    "The code can be putted in another python file separately (e.g. train.py) and be run by entering commands in terminal. (e.g. python train.py --epochs 50 --batch-size 8, which means the model will processes 8 training examples simultaneously and repeats the process 50 times)."
   ],
   "id": "fc226b0c2520f0f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_path = r'/Users/andrewyuyy/Documents/GitHub/Webcam_Detection/dataset_split/dataset.yaml' \n",
    "# Can be replaced to own path to dataset.yaml file\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command-line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Train YOLO model on custom dataset\")\n",
    "    parser.add_argument('--model', type=str, default='yolov8l.pt',help=\"Pretrained YOLO model (e.g., yolov8n.pt, yolov8s.pt, yolov8l.pt)\")\n",
    "    parser.add_argument('--epochs', type=int, default=100, help=\"Number of training epochs\")\n",
    "    parser.add_argument('--batch-size', type=int, default=16, help=\"Batch size for training\")\n",
    "    parser.add_argument('--imgsz', type=int, default=640, help=\"Image size for training and validation\")\n",
    "    parser.add_argument('--device', type=str, default='mps', help=\"Device to train on: 'cpu', 'cuda', or 'mps'\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parse arguments\n",
    "    args = parse_args()\n",
    "\n",
    "    # Verify the device\n",
    "    device = torch.device(args.device if torch.backends.mps.is_available() or args.device == 'cpu' else 'cuda')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize YOLO model\n",
    "    model = YOLO(args.model)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    model.train(\n",
    "        data=data_path,\n",
    "        epochs=args.epochs,\n",
    "        batch=args.batch_size,\n",
    "        imgsz=args.imgsz,\n",
    "        device=device\n",
    "    )\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "f9164ec7560ce534"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After the training process, the training details can be found in runs folder. The best trained model was stored under the folder named 'weights', which was named as 'best.pt'. This model can then be used in our project to detect traffic signs.",
   "id": "f6418f8da3a91d9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3. Use the model to detect traffic signs",
   "id": "1a101a70bc1988a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "By running the following code, the built-in camera of laptop was invoked to detect traffic signs. The camera used can be changed by changing the order of connected cameras captured. The cameras' order can be checked by using **FFmpeg** in the terminal. 0 here is the default built-in camera of Macbook.\n",
    "\n",
    "These lines of code will created a new window named as **'Traffic_sign_detection'**. The traffic signs captured by the camera will be detected to have a bounding box, the related class' name and also the value of confidence around."
   ],
   "id": "b7db8f0283988357"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='YOLOv8 live')\n",
    "    parser.add_argument(\n",
    "        '--webcam-resolution',\n",
    "        default=[1280,720],\n",
    "        nargs=2,\n",
    "        type=int\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    frame_width, frame_height = args.webcam_resolution\n",
    "\n",
    "    cap = cv2.VideoCapture(0) # Used the laptop built-in camera, can be changed to use other cameras connected\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\n",
    "\n",
    "    model = YOLO(r'/Users/andrewyuyy/Documents/GitHub/Webcam_Detection/runs/detect/train4/weights/best.pt') \n",
    "    # Path to best.pt file\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator(\n",
    "        thickness=2,\n",
    "        text_thickness=2,\n",
    "        text_scale=1,\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        result = model(frame)[0]\n",
    "        detections = sv.Detections.from_yolov8(result)\n",
    "        labels = [\n",
    "            f\"{model.names[class_id]}{confidence:0.2f}\"\n",
    "            for bbox, confidence, class_id\n",
    "            in zip(detections.xyxy, detections.confidence, detections.class_id)\n",
    "        ]\n",
    "\n",
    "        frame = box_annotator.annotate(\n",
    "            scene=frame,\n",
    "            detections=detections,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        cv2.imshow('Traffic_sign_detection', frame)\n",
    "\n",
    "        if (cv2.waitKey(30) == 27): # Press esc button to exist\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "f4ba65feabfad425"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Issues & Improvements",
   "id": "28323d755a8b75f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "While processing the detection, there were some issues occurred. In this section, these issues will be discussed and some possible improvements will be introduced by the end of each issues.",
   "id": "e45660de05ee30ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As shown in picture below the **U-turn** sign was correctly detected with a confidence value of **0.79**.",
   "id": "df040b5dc5ec3003"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Webcam_Detection](Test/U-turn-Upward.png)",
   "id": "d62387ac1b7f8641"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "But when the **U-turn** sign was placed in another direction, the current model cannot detect it effectively. As shown in the diagram below, the **U-turn** sign was placed upside down and it was accidentally detected as a **Park** and two **Go-straight** signs.",
   "id": "f555d45ffa7eb235"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Webcam_Detection](Test/U-turn-Upside_down.png)",
   "id": "1abadfecff0b137c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The same situation was occurred when the U-turn sign was pointed to the right and left.\n",
    "\n",
    "The main reason caused this might be the lack of training data variety. The majority of the training pictures were collected while the signs were placed upright. This caused the trained model to have weak performance when detecting the signs placed in other directions.\n",
    "\n",
    "Also, the U-turn sign data collected was less than other classes' data, which might be another reason why it has a poorer performance when taking out the detection. The picture below shows the Park sign detected while placed upright. The value of confidence was higher than that of the U-turn sign which is also placed in the upright way."
   ],
   "id": "392023ef5c02fad6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Webcam_Detection](Test/Park-Upward.png)",
   "id": "5cbf481ac1ed6bff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The picture shown below indicated that the Park sign can be correctly detected even putted direct to the right with a confidence of 0.85. The unique features contained in Park sign might be the reason why the model had a better performance in this sign detection.",
   "id": "bb0b7352865903f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Webcam_Detection](Test/Park-To_right.png)",
   "id": "ca5a470bb0d3c0b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This issue might be fixed by collecting more data of U-turn signs and signs placed in different directions, then trained the model again based on the new dataset.",
   "id": "d0c2c977f8aeb020"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ae7538da229a5403"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
